{"cells":[{"cell_type":"markdown","metadata":{"id":"z2NcobU7Ay0j"},"source":["---\n","# 使い方\n","### ①３セル目のARCHITECTURE_NAME,EXP_NUMで実験環境を指定。 \n","### ②model名/exp/EXP_NUMフォルダ内のconfigファイル内で、ハイパラを設定。\n","### ③実行\n","---"]},{"cell_type":"markdown","metadata":{"id":"a2NVQ-w5IP_G"},"source":["# 実験環境指定\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK59S8_RIEsa"},"outputs":[],"source":["import sys\n","\n","#colabかkaggleかそれ以外か\n","#colab\n","if 'google.colab' in sys.modules:\n","    ENV_FLAG='colab'\n","\n","#kaggle\n","elif 'kaggle_web_client' in sys.modules:\n","    ENV_FLAG='kaggle'\n","\n","#mac\n","elif 'ctypes.macholib.dyld' in sys.modules:\n","    ENV_FLAG='mac'\n","\n","#それ以外\n","else:\n","    ENV_FLAG=None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-g179CJCIH_0"},"outputs":[],"source":["ENV_FLAG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXZademJOBr7"},"outputs":[],"source":["#初期状態ではモジュールが入っていないので、入れる\n","if ENV_FLAG=='colab':\n","    !pip install wandb transformers\n","\n","    !pip install pytorch_lightning\n","    !pip install python-box\n","    !pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqSL-CJBeS_n"},"outputs":[],"source":["#実験環境の指定。\n","#フォルダ名を代入\n","ARCHITECTURE_NAME='external-answer-weight'\n","EXP_NUM=1\n","EXP_PATH=f'chaii/model/{ARCHITECTURE_NAME}/exp/{EXP_NUM}'\n","\n","if ENV_FLAG=='colab':\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    sys.path.append(f'/content/drive/MyDrive/kaggle/{EXP_PATH}')\n","\n","if ENV_FLAG=='kaggle':\n","    sys.path.append(f'../input/{ARCHITECTURE_NAME}-exp{EXP_NUM}')\n","\n","if ENV_FLAG=='mac':\n","    sys.path.append(f'/Volumes/GoogleDrive/マイドライブ/kaggle/{EXP_PATH}')\n","    \n","\n","from box import Box\n","import config\n","cfg=Box(vars(config.Config()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDoqnaFk6ipg"},"outputs":[],"source":["#一応、設定を目視で確認。\n","cfg"]},{"cell_type":"markdown","metadata":{"id":"hqqDoF4KW9RY"},"source":["# 共通セットアップ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sK6BU_dvWzvI"},"outputs":[],"source":["#gpuの確認\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hGnq1VVhIPIk"},"outputs":[],"source":["#kaggle.jsonの移動\n","if ENV_FLAG=='colab':\n","\n","    !mkdir -p ~/.kaggle\n","    !cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/kaggle.json\n","    !chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hwz8aSCOZCgs"},"outputs":[],"source":["import os\n","import sys\n","import copy\n","from pathlib import Path\n","import collections\n","import gc\n","gc.enable()\n","import math\n","import json\n","import time\n","import random\n","import multiprocessing\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","\n","import wandb\n","\n","import numpy as np\n","import pandas as pd\n","#notebook用tqdm。出力が綺麗になる。\n","from tqdm.notebook import tqdm\n","from sklearn import model_selection\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","import torch.optim as optim\n","from torch.utils.data import (\n","    Dataset, DataLoader,\n","    SequentialSampler, RandomSampler\n",")\n","from torch.utils.data.distributed import DistributedSampler\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer,seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping,LearningRateMonitor\n","from pytorch_lightning.loggers import WandbLogger\n","\n","\n","try:\n","    #mixed presicionを使うため、ampをインポート\n","    from apex import amp\n","    APEX_INSTALLED = True\n","except ImportError:\n","    APEX_INSTALLED = False\n","\n","import transformers\n","from transformers import (\n","    #学習済みモデルの重みのファイル名を返却\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModel,\n","    AutoTokenizer,\n","    get_cosine_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    logging,\n","    #qaタスクにおける、コンフィグとモデルの組み合わせを返却\n","    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n",")\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","\n","print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n","MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKIm6wXNwQKB"},"outputs":[],"source":["#model&tokenizer\n","if ENV_FLAG=='colab':\n","    MODEL_PATH=\"deepset/xlm-roberta-large-squad2\"\n","    TOKENIZER_PATH=\"xlm-roberta-large\"\n","#datasetは事前にDL\n","if ENV_FLAG=='kaggle':\n","    MODEL_PATH='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'\n","    TOKENIZER_PATH='../input/xlm-roberta-squad2/deepset/xlm-roberta-large-squad2'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCpjSXN5QTof"},"outputs":[],"source":["#wandbにログイン\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6Bqi5cKrSZu"},"outputs":[],"source":["def set_seed(seed):\n","    #numpyのrandomseed\n","    np.random.seed(seed)\n","    #python　randomモジュールのseed\n","    random.seed(seed)\n","    #pythonでハッシュを生成するときの乱数を固定。kerasを使うときに効果がありそう\n","    #https://qiita.com/okotaku/items/8d682a11d8f2370684c9\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    #torchで使っている乱数を固定。\n","    torch.manual_seed(seed)\n","    #単一gpuでの乱数を固定\n","    torch.cuda.manual_seed(seed)\n","    #複数gpu全ての乱数を固定\n","    torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","metadata":{"id":"qrPRyh5wQU77"},"source":["# 訓練データの用意"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XyJ3qJLcZHNL"},"outputs":[],"source":["if ENV_FLAG=='colab':\n","    train=pd.read_csv('/content/drive/MyDrive/kaggle/chaii/data/input/train.csv.zip')\n","    test=pd.read_csv('/content/drive/MyDrive/kaggle/chaii/data/input/test.csv')\n","    submission=pd.read_csv('/content/drive/MyDrive/kaggle/chaii/data/input/sample_submission.csv')\n","\n","elif ENV_FLAG=='kaggle':\n","    train=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/train.csv')\n","    test=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/test.csv')\n","    submission=pd.read_csv('../input/chaii-hindi-and-tamil-question-answering/sample_submission.csv')\n","\n","elif ENV_FLAG=='mac':\n","    train=pd.read_csv('/Volumes/GoogleDrive/マイドライブ/kaggle/chaii/data/input/train.csv.zip')\n","    test=pd.read_csv('/Volumes/GoogleDrive/マイドライブ/kaggle/chaii/data/input/test.csv')\n","    submission=pd.read_csv('/Volumes/GoogleDrive/マイドライブ/kaggle/chaii/data/input/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5i0LSP56S1PK"},"outputs":[],"source":["#拡張データセット\n","if ENV_FLAG=='colab':\n","    \n","    !kaggle datasets download -d rhtsingh/mlqa-hindi-processed \n","\n","    import zipfile\n","    with zipfile.ZipFile('/content/mlqa-hindi-processed.zip') as zip_ref:   \n","        zip_ref.extractall('/content')\n","    \n","    external_mlqa = pd.read_csv('/content/mlqa_hindi.csv')\n","    external_xquad = pd.read_csv('/content/xquad.csv')\n","\n","    external_train = pd.concat([external_mlqa, external_xquad])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuUr8u6ZIZf7"},"outputs":[],"source":["#clean data\n","#remove index\n","print(len(train))\n","mistake_id=['bc9f0d533',\n","            '1a2160a69',\n","            '632c16ba0',\n","            'e0090c270',\n","            '1b8635229',\n","            '8997bf894',\n","            '33d679522',\n","            'f22ab8d6b',\n","            '3a4db1dda']\n","\n","mistake_index=train[train.id.isin(mistake_id)].index\n","train=train.drop(mistake_index,axis=0).reset_index(drop=True)\n","print(len(train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8TNNyVPu6JQ"},"outputs":[],"source":["def apply_stratified_kfolds(data, num_splits):\n","    data[\"kfold\"] = -1\n","    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=cfg.seed)\n","    #train_indexとvalid_indexがイテレート\n","    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n","        data.loc[v_, 'kfold'] = f\n","    return data\n","\n","train = apply_stratified_kfolds(train, num_splits=cfg.num_fold)\n","#idは便宜上付与。\n","if ENV_FLAG=='colab':\n","    external_train[\"kfold\"] = -1\n","    external_train['id'] = list(np.arange(1, len(external_train)+1))\n","    train = pd.concat([train, external_train]).reset_index(drop=True)\n","\n","def convert_answers(row):\n","    return {'answer_start': [row[0]], 'text': [row[1]]}\n","\n","train['answers'] = train[['answer_start', 'answer_text']].apply(convert_answers, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEn0BFER3uWf"},"outputs":[],"source":["#各foldの比率を確認。\n","pivot=train.pivot_table(values='id',index='kfold',columns='language',aggfunc='count')\n","pivot2=pivot.apply(lambda x: x/sum(x),axis=1)\n","\n","print(pivot,pivot2,sep='\\n')"]},{"cell_type":"markdown","metadata":{"id":"1pKOtniM9_T5"},"source":["# Dataset,Dataloader関連"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Cma9yanHrOs"},"outputs":[],"source":["def prepare_train_features(example,tokenizer,max_length,doc_stride):\n","\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        features.append(feature)\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgajFIC3HrOs"},"outputs":[],"source":["def prepare_eval_features(example,tokenizer,max_length,doc_stride):\n","\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n","    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n","\n","    features = []\n","    for i, offsets in enumerate(offset_mapping):\n","        feature = {}\n","\n","        input_ids = tokenized_example[\"input_ids\"][i]\n","        attention_mask = tokenized_example[\"attention_mask\"][i]\n","\n","        feature[\"example_id\"] = example['id']\n","        feature['context'] = example['context']\n","        feature['question'] = example['question']\n","\n","        feature['input_ids'] = input_ids\n","        feature['attention_mask'] = attention_mask\n","        feature['offset_mapping'] = offsets\n","\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","        sequence_ids = tokenized_example.sequence_ids(i)\n","\n","        sample_index = sample_mapping[i]\n","        answers = example[\"answers\"]\n","\n","        if len(answers[\"answer_start\"]) == 0:\n","            feature[\"start_position\"] = cls_index\n","            feature[\"end_position\"] = cls_index\n","        else:\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                feature[\"start_position\"] = cls_index\n","                feature[\"end_position\"] = cls_index\n","            else:\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                feature[\"start_position\"] = token_start_index - 1\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                feature[\"end_position\"] = token_end_index + 1\n","\n","        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]      \n","        \n","        features.append(feature)\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RSWpLxKGHrOt"},"outputs":[],"source":["def prepare_test_features(example, tokenizer,max_length,doc_stride):\n","    example[\"question\"] = example[\"question\"].lstrip()\n","    \n","    tokenized_example = tokenizer(\n","        example[\"question\"],\n","        example[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    features = []\n","    #各spanごとに、辞書形で情報を格納\n","    for i in range(len(tokenized_example[\"input_ids\"])):\n","        feature = {}\n","        feature[\"example_id\"] = example['id']\n","        feature['context'] = example['context']\n","        feature['question'] = example['question']\n","        feature['input_ids'] = tokenized_example['input_ids'][i]\n","        feature['attention_mask'] = tokenized_example['attention_mask'][i]\n","        feature['offset_mapping'] = tokenized_example['offset_mapping'][i]\n","        #sequence_idsがNoneはスペシャルトーケンの場合。この場合は、sequence_idsを0にする。\n","        feature['sequence_ids'] = [0 if i is None else i for i in tokenized_example.sequence_ids(i)]\n","        features.append(feature)\n","    return features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jUwiWRleHrOt"},"outputs":[],"source":["class ChaiiDataset(Dataset):\n","    def __init__(self, features, mode='train'):\n","        super(ChaiiDataset, self).__init__()\n","        self.features = features\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.features)\n","    \n","    #trainとtestで出力するデータを分けている（必要なものが異なるため）\n","    def __getitem__(self, index):   \n","        feature = self.features[index]\n","        if self.mode == 'train':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                #'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","        elif self.mode == 'eval':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'example_id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question'],\n","                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n","                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n","            }\n","            \n","        elif self.mode == 'test':\n","            return {\n","                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n","                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n","                'offset_mapping':feature['offset_mapping'],\n","                'sequence_ids':feature['sequence_ids'],\n","                'id':feature['example_id'],\n","                'context': feature['context'],\n","                'question': feature['question']\n","            }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_nXGEqeHrOu"},"outputs":[],"source":["#dataloader用　上記2つを使用して作成。jaccardをmonitorするためのdataloaderも作成。\n","def make_loader(\n","    data, \n","    tokenizer,\n","    max_length,\n","    doc_stride,\n","    train_batch_size,\n","    eval_batch_size,\n","    fold\n","):\n","    train_df, valid_df = data[data['kfold']!=fold], data[data['kfold']==fold].reset_index(drop=True)\n","    \n","    train_features =[]\n","    valid_features =[]\n","\n","    for i, row in tqdm(train_df.iterrows(),total=len(train_df)):\n","        train_features += prepare_train_features(row,tokenizer,max_length,doc_stride)\n","\n","    for i, row in tqdm(valid_df.iterrows(),total=len(valid_df)):\n","        valid_features += prepare_eval_features(row,tokenizer,max_length,doc_stride)\n","\n","\n","    train_dataset = ChaiiDataset(train_features)\n","    valid_dataset = ChaiiDataset(valid_features,mode='eval')\n","\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        batch_size=train_batch_size,\n","        shuffle=True,\n","        num_workers=2,\n","        pin_memory=True,\n","        drop_last=False \n","    )\n","\n","    valid_dataloader = DataLoader(\n","        valid_dataset,\n","        batch_size=eval_batch_size, \n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=True, \n","        drop_last=False\n","    )\n","\n","    return train_dataloader, valid_dataloader,valid_df,valid_features"]},{"cell_type":"markdown","metadata":{"id":"rL3yZUgwHrOu"},"source":["\n","\n","# 損失"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yk4U4-py1fs"},"outputs":[],"source":["# def loss_fn(preds, labels):\n","#     start_preds, end_preds = preds\n","#     start_labels, end_labels = labels\n","\n","#     start_loss = nn.CrossEntropyLoss()(start_preds, start_labels)\n","#     end_loss = nn.CrossEntropyLoss()(end_preds, end_labels)\n","#     total_loss = (start_loss + end_loss) / 2\n","\n","#     return total_loss\n","\n","def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","def weight_loss_fn(preds, labels,tokenizer,alpha=2):\n","    start_preds, end_preds = preds\n","    start_labels, end_labels = labels\n","\n","    start_loss = nn.CrossEntropyLoss()(start_preds, start_labels)\n","    end_loss = nn.CrossEntropyLoss()(end_preds, end_labels)\n","    total_loss = (start_loss + end_loss) / 2\n","\n","    #答えを持っているとき、損失を増やすように修正\n","    have_answer=start_labels!=tokenizer.cls_token_id\n","    \n","    weight_=torch.tensor([1.]).repeat(len(start_labels))\n","    weight_=weight_.to('cuda')\n","    weight_[have_answer]*=alpha\n","    \n","    #答えがあるサンプルについては、重みをalpha倍し、最終のtotal_lossを算出\n","    total_loss=(total_loss*weight_.sum())/len(weight_)\n","\n","    return total_loss"]},{"cell_type":"markdown","metadata":{},"source":["# 推論用関数。valid_dataのjaccard係数を算出するために定義"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVDr3PbMV6I-"},"outputs":[],"source":["def postprocess_qa_predictions(examples, features1, raw_predictions,tokenizer,n_best_size = 20, max_answer_length = 30):\n","        #offsetのNoneへの変換を、元のfeaturesに反映させないようにする。\n","        features=copy.deepcopy(features1)\n","        all_start_logits, all_end_logits = raw_predictions\n","        \n","        #各exampleに対して、どれくらいfeatureがあるか。各exampleとfeatureのindexは０始まりに修正されている。なお、リストをvalueとした、デフォルトdict形式\n","        example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","        features_per_example = collections.defaultdict(list)\n","        for i, feature in enumerate(features):\n","            features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","        predictions = collections.OrderedDict()\n","\n","        print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","        for example_index, example in examples.iterrows():\n","            feature_indices = features_per_example[example_index]\n","\n","            min_null_score = None\n","            valid_answers = []\n","            \n","            context = example[\"context\"]\n","            for feature_index in feature_indices:\n","                start_logits = all_start_logits[feature_index]\n","                end_logits = all_end_logits[feature_index]\n","\n","                sequence_ids = features[feature_index][\"sequence_ids\"]\n","                context_index = 1\n","\n","                features[feature_index][\"offset_mapping\"] = [\n","                    (o if sequence_ids[k] == context_index else None)\n","                    for k, o in enumerate(features[feature_index][\"offset_mapping\"])\n","                ]\n","                offset_mapping = features[feature_index][\"offset_mapping\"]\n","\n","                cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","                feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","                if min_null_score is None or min_null_score < feature_null_score:\n","                    min_null_score = feature_null_score\n","\n","                start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","                end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","                #上位２０個数のうち、妥当なもののみを候補とする。\n","                for start_index in start_indexes:\n","                    for end_index in end_indexes:\n","                        if (\n","                            start_index >= len(offset_mapping)\n","                            or end_index >= len(offset_mapping)\n","                            or offset_mapping[start_index] is None\n","                            or offset_mapping[end_index] is None\n","                        ):\n","                            continue\n","                        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","                        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","                            continue\n","\n","                        start_char = offset_mapping[start_index][0]\n","                        end_char = offset_mapping[end_index][1]\n","                        valid_answers.append(\n","                            {\n","                                \"score\": start_logits[start_index] + end_logits[end_index],\n","                                \"text\": context[start_char: end_char]\n","                            }\n","                        )\n","            #impossible_answerの判定はおこなっていない。（元：https://colab.research.google.com/drive/13QRHItm8dLliHUFckUwppsLK_U2vCmgS#scrollTo=rdckkKKuj48S）\n","            if len(valid_answers) > 0:\n","                best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","            else:\n","                best_answer = {\"text\": \"\", \"score\": 0.0}\n","            \n","            predictions[example[\"id\"]] = best_answer[\"text\"]\n","            \n","        return predictions\n"]},{"cell_type":"markdown","metadata":{"id":"750adN_LroBO"},"source":["# tokenizer,seedの設定"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V71rUSbrnCm"},"outputs":[],"source":["tokenizer=AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n","#seed固定\n","set_seed(cfg.seed)"]},{"cell_type":"markdown","metadata":{"id":"QOW11KEyzind"},"source":["# MODEL(pytorch-lightning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSrO217NGIOG"},"outputs":[],"source":["class ChaiiModel(pl.LightningModule):\n","    def __init__(\n","        self,\n","        model_path,\n","        tokenizer,\n","        weight_decay,\n","        learning_rate,\n","        epoch,\n","        train_batch_size,\n","        valid_batch_size,\n","        warmup_ratio,\n","        gradient_accumulation_steps,\n","        t_dataloader,\n","        v_dataloader,\n","        valid_df,\n","        valid_features\n","    ):\n","\n","        super().__init__()\n","        #gradient_accumulateのため、マニュアル\n","        self.automatic_optimization = False\n","\n","        #config\n","        self.weight_decay=weight_decay\n","        self.learning_rate=learning_rate\n","        self.epoch=epoch\n","        self.train_batch_size=train_batch_size\n","        self.valid_batch_size=valid_batch_size\n","        self.warmup_ratio=warmup_ratio\n","        self.gradient_accumulation_steps=gradient_accumulation_steps\n","\n","        #tokenizer\n","        self.tokenizer=tokenizer\n","\n","        #model\n","        self.model_config=AutoConfig.from_pretrained(model_path)\n","        self.model_config.update(\n","            {\n","                \"output_hidden_states\": True,\n","                \"hidden_dropout_prob\": 0.1,\n","                #\"layer_norm_eps\": ,\n","                #\"add_pooling_layer\": False,\n","            }\n","        )\n","        self.model=AutoModel.from_pretrained(model_path,config=self.model_config)\n","        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n","        self._init_weights(self.qa_outputs)\n","        \n","        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n","\n","        #dataloader\n","        self._train_dataloader=t_dataloader\n","        self._valid_dataloader=v_dataloader\n","\n","        #valid_df\n","        self.valid_df=valid_df\n","        #valid_features\n","        self.valid_features=valid_features\n","\n","        #save_hyperparameters\n","        self.save_hyperparameters('weight_decay',\n","                                    'learning_rate',\n","                                    'epoch',\n","                                    'train_batch_size',\n","                                    'valid_batch_size',\n","                                    'warmup_ratio',\n","                                    'gradient_accumulation_steps')\n","    \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","    def configure_optimizers(self):\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","            ]\n","        optimizer = AdamW(\n","                optimizer_grouped_parameters,\n","                lr=self.learning_rate,\n","            )\n","        \n","        num_training_steps=math.ceil(len(self._train_dataloader)/self.gradient_accumulation_steps)*self.epoch\n","        num_warmup_steps=num_training_steps*self.warmup_ratio\n","\n","        scheduler=get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_training_steps=num_training_steps,\n","            num_warmup_steps=num_warmup_steps,\n","        )\n","\n","        return {'optimizer':optimizer,'lr_scheduler':scheduler}\n","\n","    def forward(self):\n","        pass\n","\n","    def _shared_step(self, batch):\n","        input_ids=batch['input_ids']\n","        attention_mask=batch['attention_mask']\n","        \n","        outputs=self.model(input_ids,attention_mask)\n","        last_hidden_state=outputs.last_hidden_state\n","\n","        #batch*max_length*2の行列\n","        qa_logits=self.qa_outputs(last_hidden_state)\n","\n","        #横軸方向(dim=2)に、chunk_size=1で分割→batch*max_length*1の行列が2つできる。\n","        start_logits, end_logits=qa_logits.split(1,dim=-1)\n","\n","        #batch*max_lengthの２次元行列に変換\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","\n","        return (start_logits,end_logits)\n","\n","    #gradient_accumulateを加味しているため、マニュアルbackward\n","    def training_step(self,batch, batch_idx):\n","        \n","        opt = self.optimizers()\n","        sch = self.lr_schedulers()\n","\n","        #import pdb; pdb.set_trace()\n","        start_logits, end_logits=self._shared_step(batch)\n","        start_position=batch['start_position']\n","        end_position=batch['end_position']\n","\n","        \n","        #loss=loss_fn((start_logits,end_logits),(start_position,end_position))\n","        loss=weight_loss_fn((start_logits,end_logits),(start_position,end_position),self.tokenizer,alpha=2)\n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n","\n","        #if average \n","        loss = loss / self.gradient_accumulation_steps\n","\n","        #backward\n","        self.manual_backward(loss)\n","\n","         # accumulate gradients of `n` batches\n","        if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n","            opt.step()\n","            sch.step()\n","            opt.zero_grad()\n","\n","    #def training_step_end(...)\n","    #def training_epoch_end(...)\n","\n","    def validation_step(self, batch, batch_idx):\n","\n","        start_logits, end_logits=self._shared_step(batch)\n","\n","        start_position=batch['start_position']\n","        end_position=batch['end_position']\n","        \n","        loss=weight_loss_fn((start_logits,end_logits),(start_position,end_position),self.tokenizer,alpha=2)\n","        self.log_dict({\"val_loss\":loss},on_step=True,on_epoch=True,prog_bar=True)\n","\n","        return [start_logits,end_logits]\n","    def validation_epoch_end(self,val_step_outputs):\n","        #jaccard計算。logitはndarrayに直してる。（postprocess用に）\n","        #from IPython.core.debugger import Pdb; Pdb().set_trace()\n","\n","        all_start_logits, all_end_logits=[],[]\n","        for out in val_step_outputs:\n","            all_start_logits.append(out[0].to('cpu').detach().numpy().tolist())\n","            all_end_logits.append(out[1].to('cpu').detach().numpy().tolist())\n","        all_start_logits=np.vstack(all_start_logits)\n","        all_end_logits=np.vstack(all_end_logits)\n","        \n","        predictions=postprocess_qa_predictions(self.valid_df,self.valid_features,(all_start_logits, all_end_logits),self.tokenizer)\n","        res=self.valid_df[['id','answer_text']]\n","        res['prediction']=res['id'].map(predictions)\n","        all_jaccard=res[['answer_text', 'prediction']].apply(lambda x: jaccard(x[0],x[1]), axis=1)\n","        epoch_jaccard=np.mean(all_jaccard)\n","        self.log_dict({\"jaccard\":epoch_jaccard})\n","\n","    def train_dataloader(self):\n","        return self._train_dataloader\n","\n","    def val_dataloader(self):\n","        return self._valid_dataloader\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# 訓練実行"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mVx4lEEUYlw"},"outputs":[],"source":["for fold in range(cfg.num_fold):\n","\n","    print(f'FOLD: {fold+1}')\n","    train_dataloader,val_dataloader,valid_df,valid_features=\\\n","        make_loader(train,\n","                    tokenizer,\n","                    cfg.max_length,\n","                    cfg.doc_stride,\n","                    cfg.train_batch_size, \n","                    cfg.valid_batch_size,\n","                    fold=fold)\n","    \n","    #setup\n","    model=ChaiiModel(\n","            MODEL_PATH,\n","            tokenizer,\n","            cfg.weight_decay,\n","            cfg.lr,\n","            cfg.epoch,\n","            cfg.train_batch_size,\n","            cfg.valid_batch_size,\n","            cfg.warmup_ratio,\n","            cfg.gradient_accumulation_steps,\n","            train_dataloader,\n","            val_dataloader,\n","            valid_df,\n","            valid_features\n","        )\n","\n","    checkpoint_callback = ModelCheckpoint(monitor='jaccard',\n","                                save_top_k=1,\n","                                save_weights_only=True,\n","                                dirpath=cfg.param_dir,\n","                                filename=f'{fold+1}fold/{fold+1}fold',\n","                                verbose=False,\n","                                mode='max')\n","    \n","    #LRmonitor\n","    learning_rate_monitor=LearningRateMonitor(logging_interval='step')\n","\n","    early_stopping = EarlyStopping(monitor='jaccard',mode='max',patience=6)\n","    \n","    #kfoldはgroupで一元管理。\n","    wandb_logger = WandbLogger(project=cfg.architecture_name,\n","                            name=f'{fold+1}fold',\n","                            group=cfg.exp_name,\n","                            )\n","\n","    trainer = pl.Trainer(max_epochs=cfg.epoch,\n","                        checkpoint_callback=True, \n","                        gpus=1, \n","                        #val_check回数。\n","                        val_check_interval=0.2,\n","                        deterministic=True,#https://kutohonn.hatenablog.com/entry/2021/01/04/232434より。SEEDを固定するためにオプション。\n","                        callbacks=[checkpoint_callback,learning_rate_monitor,early_stopping],\n","                        #fast_dev_run=True,\n","                        logger=wandb_logger,\n","                        #推論関数を適切に実行させるために以下を設定。\n","                        num_sanity_val_steps=0\n","                        )\n","    \n","    print('training start')\n","    trainer.fit(model)\n","\n","\n","    #これを実行しないと、新しいものが立ち上がらない\n","    wandb.finish()\n","\n","    #RAM確保\n","    del model\n","    gc.collect()\n","    torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eyJXkkBj1X-A"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train.ipynb","private_outputs":true,"provenance":[]},"interpreter":{"hash":"b07164df2be8f17e301439c03265fdfe021f9bdab5f3c2c0e7e2b21533a5e31b"},"kernelspec":{"display_name":"Python 3.9.7 64-bit ('analytics': venv)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
